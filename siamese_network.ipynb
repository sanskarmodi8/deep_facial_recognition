{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 22:43:13.234494: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sanskar/Desktop/deep_facial_recognition/.conda/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-11-17 22:43:13.234513: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import uuid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Paths and create folder structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup paths\n",
    "POS_PATH = os.path.join('data', 'positiveimgs')\n",
    "NEG_PATH = os.path.join('data', 'negativeimgs')\n",
    "ANC_PATH = os.path.join('data', 'anchorimgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structures\n",
    "os.makedirs(POS_PATH, exist_ok=True)\n",
    "os.makedirs(NEG_PATH, exist_ok=True)\n",
    "os.makedirs(ANC_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative images -- from the Labelled faces in the wild dataset (https://shorturl.at/aitN9) (all images are in the 250 x 250 format)\n",
    "# anchor images -- from the webcam\n",
    "# positive images -- from the webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting Negative images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#install tar file\n",
    "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untar the file\n",
    "!tar -xvzf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all images to the negative folder\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting Positive and Anchor images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened(): # while webcam is open\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    frame = frame[160:160+250, 200:200+250, :] # crop the frame to 250 x 250 with all color channels\n",
    "    \n",
    "    # collect anchors\n",
    "    if cv2.waitKey(1) & 0xFF == ord('a'): # if 'a' is pressed while it waits for a millisecond\n",
    "        imgname = os.path.join(ANC_PATH, str(uuid.uuid1()) + '.jpg') # create a unique name for the image # uuid.uuid1 generates a random uuid (universally unique id)\n",
    "        cv2.imwrite(imgname, frame) # write out anchor image\n",
    "    \n",
    "    # collect positives\n",
    "    if cv2.waitKey(1) & 0xFF == ord('p'): # if 'p' is pressed while it waits for a millisecond\n",
    "        imgname = os.path.join(POS_PATH, str(uuid.uuid1()) + '.jpg')\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # if 'q' is pressed while it waits for a millisecond\n",
    "        break\n",
    "    \n",
    "cap.release() # release the webcam\n",
    "cv2.destroyAllWindows() # close all windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 22:44:27.142640: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-11-17 22:44:27.143985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-11-17 22:44:27.166754: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-11-17 22:44:27.166779: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Lapy): /proc/driver/nvidia/version does not exist\n",
      "2023-11-17 22:44:27.167665: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-17 22:44:27.168425: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "anchor = tf.data.Dataset.list_files(os.path.join(ANC_PATH, '*.jpg')).take(359) # take 359 images from the anchor folder #we choose 359 because we have a minimum of 359 images in the anchor, positive and negative data folders\n",
    "positive = tf.data.Dataset.list_files(os.path.join(POS_PATH, '*.jpg')).take(359) # take 359 images from the positive folder\n",
    "negative = tf.data.Dataset.list_files(os.path.join(NEG_PATH, '*.jpg')).take(359) # take 359 images from the negative folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Labelled dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConcatenateDataset shapes: ((), (), ()), types: (tf.string, tf.string, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "# (anchor, positive) => 1\n",
    "# (anchor, negative) => 0\n",
    "\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor))))) # create a dataset of (anchor, positive, 1) # from_tensor_slices creates a dataset from a tensor\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor))))) # create a dataset of (anchor, negative, 0)\n",
    "data = positives.concatenate(negatives) # concatenate the two datasets\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'data/anchorimgs/b14ba46f-8486-11ee-82cd-3b68759fe6e2.jpg', b'data/positiveimgs/3bf18032-8485-11ee-82cd-3b68759fe6e2.jpg', 1.0)\n"
     ]
    }
   ],
   "source": [
    "samples = data.as_numpy_iterator()\n",
    "print(samples.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and Data loader pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path) # read the image\n",
    "    img = tf.image.decode_jpeg(byte_img) # decode the image\n",
    "    img = tf.image.resize(img, (100, 100)) # resize the image to 100x100\n",
    "    img = img/255.0 # normalize the image\n",
    "    return img\n",
    "\n",
    "def preprocess_twin(inputimg, valimg, label):\n",
    "    return (preprocess(inputimg), preprocess(valimg)), label\n",
    "\n",
    "data = data.map(preprocess_twin) # map the preprocess_twin function to the dataset # data gets converted from ( a, p, l) to ((a,  p) , l) due to the return statement in the preprocess_twin function\n",
    "data = data.cache() # cache the dataset\n",
    "data = data.shuffle(1024) # shuffle the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing 5 samples from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.take(5)\n",
    "\n",
    "fig, axs = plt.subplots(5, 2, figsize=(10, 20))\n",
    "for i, sample in enumerate(samples):\n",
    "    axs[i, 0].imshow(sample[0][0])\n",
    "    axs[i, 0].set_title('Anchor Image')\n",
    "    axs[i, 1].imshow(sample[0][1])\n",
    "    axs[i, 1].set_title('Validation Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training partition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.take(round(0.7 * len(data))) # take 70% of the data for training\n",
    "train_data = train_data.batch(16) # batch the data\n",
    "train_data = train_data.prefetch(8) # prefetch the data for faster processing and avoiding bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing partition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.skip(round(0.7 * len(data))) # skip 70% of the data for testing\n",
    "test_data = test_data.take(round(0.3 * len(data))) # take 30% of the data for testing\n",
    "test_data = test_data.batch(16) # batch the data\n",
    "test_data = test_data.prefetch(8) # prefetch the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build embedding layer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    \n",
    "    inp = Input(shape=(100, 100, 3)) # input layer\n",
    "    \n",
    "    # Frist Block\n",
    "    c1 = Conv2D(64, (10, 10), activation='relu')(inp) # convolutional layer\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1) # max pooling layer\n",
    "    \n",
    "    # Second Block\n",
    "    c2 = Conv2D(128, (7, 7), activation='relu')(m1) # convolutional layer\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2) # max pooling layer\n",
    "    \n",
    "    # Third Block\n",
    "    c3 = Conv2D(128, (4, 4), activation='relu')(m2) # convolutional layer\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3) # max pooling layer\n",
    "    \n",
    "    # Final Block\n",
    "    c4 = Conv2D(256, (4, 4), activation='relu')(m3) # convolutional layer\n",
    "    f1 = Flatten()(c4) # flatten the output\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1) # dense layer\n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding') # return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 100, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 91, 91, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 40, 40, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 17, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding = make_embedding() # create the model\n",
    "embedding.summary() # print the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Distance layer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer): \n",
    "        \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding) # return the absolute difference between the two inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Siamese model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    \n",
    "    # handle inputs \n",
    "    anchor_img = Input(name=\"anchor_img\", shape=(100, 100, 3))\n",
    "    validation_img = Input(name=\"val_img\", shape=(100, 100, 3)) \n",
    "    \n",
    "    # combining the distance component and the embedding component\n",
    "    dist_layer = L1Dist() # create the distance layer\n",
    "    dist_layer._name = 'distance' # name the layer\n",
    "    distances = dist_layer(embedding(anchor_img), embedding(validation_img)) # calculate the distance between the two embeddings\n",
    "    \n",
    "    #classification component\n",
    "    classification = Dense(1, activation='sigmoid')(distances) # create the classification layer\n",
    "    \n",
    "    return Model(inputs=[anchor_img, validation_img], outputs=[classification], name='siameseNetwork') # return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_img (InputLayer)         [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "val_img (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Functional)          (None, 4096)         38960448    anchor_img[0][0]                 \n",
      "                                                                 val_img[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "distance (L1Dist)               (None, 4096)         0           embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            4097        distance[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_net = make_siamese_model() # create the model\n",
    "siamese_net.summary() # print the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup loss and optimizers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.keras.losses.BinaryCrossentropy() # create the loss function\n",
    "opt = tf.keras.optimizers.Adam(1e-4) # create the optimizer # 1e-4 = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Checkpoints :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, model=siamese_net) # create a checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build train step function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    with tf.GradientTape() as tape: # record all operations done to the inputs\n",
    "        X = batch[0][:2] # get the anchor and pos/neg images\n",
    "        y = batch[1] # get the labels\n",
    "        \n",
    "        # forward prop \n",
    "        yhat = siamese_net(X, training=True) # get the predictions\n",
    "        loss = binary_cross_loss(y, yhat) # calculate the loss\n",
    "    \n",
    "    # back prop\n",
    "    grads = tape.gradient(loss, siamese_net.trainable_variables) # calculate the gradients\n",
    "    opt.apply_gradients(zip(grads, siamese_net.trainable_variables)) # apply the gradients\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build training loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Creating a metric object \n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            loss = train_step(batch)\n",
    "            yhat = siamese_net.predict(batch[0][:2])\n",
    "            p.update_state(batch[1], yhat) \n",
    "            r.update_state(batch[1], yhat)\n",
    "            progbar.update(idx+1)\n",
    "        print(\"LOSS: \" + str(loss.numpy()) + \" RECALL: \" + str(r.result().numpy()) + \" PRECISION: \" + str(p.result().numpy()))\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/50\n",
      "32/32 [==============================] - 272s 9s/step\n",
      "LOSS: 0.11887312 RECALL: 0.9147287 PRECISION: 0.9874477\n",
      "\n",
      " Epoch 2/50\n",
      "32/32 [==============================] - 275s 9s/step\n",
      "LOSS: 0.056339644 RECALL: 0.97131145 PRECISION: 0.9957983\n",
      "\n",
      " Epoch 3/50\n",
      "32/32 [==============================] - 275s 9s/step\n",
      "LOSS: 0.0002914805 RECALL: 0.9882353 PRECISION: 0.99604744\n",
      "\n",
      " Epoch 4/50\n",
      "32/32 [==============================] - 275s 9s/step\n",
      "LOSS: 0.027512914 RECALL: 0.988 PRECISION: 0.99596775\n",
      "\n",
      " Epoch 5/50\n",
      "32/32 [==============================] - 275s 9s/step\n",
      "LOSS: 0.0020919587 RECALL: 0.9922179 PRECISION: 0.9922179\n",
      "\n",
      " Epoch 6/50\n",
      "32/32 [==============================] - 276s 9s/step\n",
      "LOSS: 0.31449324 RECALL: 0.9919355 PRECISION: 1.0\n",
      "\n",
      " Epoch 7/50\n",
      "32/32 [==============================] - 276s 9s/step\n",
      "LOSS: 0.09957038 RECALL: 0.96428573 PRECISION: 0.99590164\n",
      "\n",
      " Epoch 8/50\n",
      "32/32 [==============================] - 277s 9s/step\n",
      "LOSS: 0.2540321 RECALL: 0.946281 PRECISION: 0.9956522\n",
      "\n",
      " Epoch 9/50\n",
      "32/32 [==============================] - 276s 9s/step\n",
      "LOSS: 0.011796054 RECALL: 0.95752895 PRECISION: 0.992\n",
      "\n",
      " Epoch 10/50\n",
      "32/32 [==============================] - 276s 9s/step\n",
      "LOSS: 0.13321283 RECALL: 0.9922179 PRECISION: 0.9922179\n",
      "\n",
      " Epoch 11/50\n",
      "32/32 [==============================] - 276s 9s/step\n",
      "LOSS: 0.18760954 RECALL: 0.983871 PRECISION: 1.0\n",
      "\n",
      " Epoch 12/50\n",
      "32/32 [==============================] - 276s 9s/step\n",
      "LOSS: 0.01487653 RECALL: 0.99230766 PRECISION: 0.99230766\n",
      "\n",
      " Epoch 13/50\n",
      "32/32 [==============================] - 278s 9s/step\n",
      "LOSS: 0.063454255 RECALL: 0.9919028 PRECISION: 1.0\n",
      "\n",
      " Epoch 14/50\n",
      "32/32 [==============================] - 276s 9s/step\n",
      "LOSS: 9.9472716e-05 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 15/50\n",
      "32/32 [==============================] - 276s 9s/step\n",
      "LOSS: 0.0018794456 RECALL: 0.9959514 PRECISION: 1.0\n",
      "\n",
      " Epoch 16/50\n",
      "32/32 [==============================] - 277s 9s/step\n",
      "LOSS: 0.011088938 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 17/50\n",
      "32/32 [==============================] - 277s 9s/step\n",
      "LOSS: 0.03436643 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 18/50\n",
      "32/32 [==============================] - 277s 9s/step\n",
      "LOSS: 0.036211785 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 19/50\n",
      "32/32 [==============================] - 278s 9s/step\n",
      "LOSS: 5.168633e-06 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 20/50\n",
      "32/32 [==============================] - 278s 9s/step\n",
      "LOSS: 0.03369394 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 21/50\n",
      "32/32 [==============================] - 267s 8s/step\n",
      "LOSS: 0.00010049836 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 22/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 0.0013048041 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 23/50\n",
      "32/32 [==============================] - 264s 8s/step\n",
      "LOSS: 1.3964585e-06 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 24/50\n",
      "32/32 [==============================] - 264s 8s/step\n",
      "LOSS: 0.0023952532 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 25/50\n",
      "32/32 [==============================] - 264s 8s/step\n",
      "LOSS: 0.0007072179 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 26/50\n",
      "32/32 [==============================] - 262s 8s/step\n",
      "LOSS: 0.000111396424 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 27/50\n",
      "32/32 [==============================] - 266s 8s/step\n",
      "LOSS: 0.001137938 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 28/50\n",
      "32/32 [==============================] - 264s 8s/step\n",
      "LOSS: 5.761525e-05 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 29/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 0.0010550569 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 30/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 3.13384e-05 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 31/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 0.00033685163 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 32/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 2.3060344e-05 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 33/50\n",
      "32/32 [==============================] - 264s 8s/step\n",
      "LOSS: 1.2772428e-07 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 34/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 5.977611e-06 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 35/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 7.028113e-05 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 36/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 2.8245344e-05 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 37/50\n",
      "32/32 [==============================] - 262s 8s/step\n",
      "LOSS: 1.70299e-08 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 38/50\n",
      "32/32 [==============================] - 261s 8s/step\n",
      "LOSS: 0.000556091 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 39/50\n",
      "32/32 [==============================] - 262s 8s/step\n",
      "LOSS: 0.0008472341 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 40/50\n",
      "32/32 [==============================] - 261s 8s/step\n",
      "LOSS: 1.7661096e-05 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 41/50\n",
      "32/32 [==============================] - 262s 8s/step\n",
      "LOSS: 1.4475417e-07 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 42/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 0.00040993094 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 43/50\n",
      "32/32 [==============================] - 260s 8s/step\n",
      "LOSS: 7.40802e-07 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 44/50\n",
      "32/32 [==============================] - 260s 8s/step\n",
      "LOSS: 3.4059802e-08 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 45/50\n",
      "32/32 [==============================] - 261s 8s/step\n",
      "LOSS: 0.00017172362 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 46/50\n",
      "32/32 [==============================] - 262s 8s/step\n",
      "LOSS: 0.00013384446 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 47/50\n",
      "32/32 [==============================] - 262s 8s/step\n",
      "LOSS: 0.00015429834 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 48/50\n",
      "32/32 [==============================] - 262s 8s/step\n",
      "LOSS: 1.958439e-07 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 49/50\n",
      "32/32 [==============================] - 261s 8s/step\n",
      "LOSS: 3.653196e-05 RECALL: 1.0 PRECISION: 1.0\n",
      "\n",
      " Epoch 50/50\n",
      "32/32 [==============================] - 263s 8s/step\n",
      "LOSS: 0.000111546775 RECALL: 1.0 PRECISION: 1.0\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall values for all th 14 batches in the test data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Batch no 0 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 1 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 2 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 3 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 4 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 5 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 6 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 7 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 8 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 9 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 10 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 11 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 12 => Precision: 1.0 , Recall: 1.0\n",
      "Test data Batch no 13 => Precision: 1.0 , Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "precision = Precision()\n",
    "recall = Recall()\n",
    "for id, batch in enumerate(test_data):\n",
    "    test_input = batch[0][0]\n",
    "    test_val = batch[0][1]\n",
    "    y_true = batch[1]\n",
    "    yhat = siamese_net.predict([test_input, test_val])\n",
    "    precision.update_state(y_true, yhat)\n",
    "    recall.update_state(y_true, yhat)\n",
    "    print(f\"Test data Batch no {id} => Precision: {precision.result().numpy()} , Recall: {recall.result().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "siamese_net.save('siamesemodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('siamesemodel.h5', custom_objects={'L1Dist': L1Dist}) # load the model # custom_objects is used to load the custom layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opencv real time facial recognition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the name and the probability of the input image which matches the anchor image the most\n",
    "# it assumes that you have a folder called 'recognition_data' in the same directory as this notebook and that folder contains a folder called 'input_image' which contains the input image and a folder called 'verification_imgs' which contains the images of different persons and the imaes are named after the person's name\n",
    "\n",
    "def recognize():\n",
    "    pred_name = ''\n",
    "    max_prob = 0\n",
    "    for name in os.listdir(os.path.join('recognition_data', 'verification_imgs')):\n",
    "        img = os.path.join('recognition_data', 'verification_imgs', name)\n",
    "        img = preprocess(img)\n",
    "        val_img = os.path.join('recognition_data', 'input_image', 'input.jpg')\n",
    "        val_img = preprocess(val_img)\n",
    "        result = model.predict(list(np.expand_dims([img, val_img], axis=1)))\n",
    "        if result>max_prob:\n",
    "            max_prob = result\n",
    "            pred_name = name\n",
    "    return pred_name, max_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanskar. [[0.4971676]]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    frame = frame[160:160+250, 200:200+250, :]\n",
    "    \n",
    "    # recognition trigger \n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        cv2.imwrite(os.path.join(\"recognition_data\", \"input_image\", \"input.jpg\"), frame) # save the input image\n",
    "        name, prob = recognize()\n",
    "        print(str(name)[:-4], prob)\n",
    "    \n",
    "    cv2.imshow('Recognize', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do verification :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required folders :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(\"verification_data\", \"verification_images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(\"verification_data\", \"input_images\"), exist_ok=True)\n",
    "\n",
    "# copy some positive images to the verification folder\n",
    "for i in range(30):\n",
    "    imgname = os.path.join(POS_PATH, os.listdir(POS_PATH)[i])\n",
    "    newname = os.path.join(\"verification_data\", \"verification_images\", str(uuid.uuid1()) + '.jpg')\n",
    "    shutil.copy(imgname, newname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    \n",
    "    #detection threshold : metric above which a prediction is considered as positive\n",
    "    #verification threshold : min proportion of (positive predictions))/(total positive samples) required to verify a person\n",
    "    \n",
    "    results=[]\n",
    "    for img in os.listdir(os.path.join(\"verification_data\", \"verification_images\")):\n",
    "        input_img = preprocess(os.path.join(\"verification_data\", \"input_images\", \"input_image.jpg\"))\n",
    "        val_img = preprocess(os.path.join(\"verification_data\", \"verification_images\", img))\n",
    "        result = model.predict(list(np.expand_dims([input_img, val_img], axis=1))) #list(np.expand_dims([input_img, val_img], axis=1)) will first expand the dimensions of the input and val images from (100, 100, 3) to (1, 100, 100, 3) by adding it into another set of parenthesis and then convert it to a list # we added the extra dimension because we are passing a single input\n",
    "        results.append(result)\n",
    "        \n",
    "    detection = np.sum(np.array(results) > detection_threshold) # calculate the number of positive predictions\n",
    "    verification = detection/len(os.listdir(os.path.join(\"verification_data\", \"verification_images\"))) # calculate the proportion of positive predictions\n",
    "    verified = verification > verification_threshold # check if the proportion of positive predictions is greater than the verification threshold\n",
    "    \n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opencv real time verification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    frame = frame[160:160+250, 200:200+250, :]\n",
    "    \n",
    "    # verification trigger \n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        cv2.imwrite(os.path.join(\"verification_data\", \"input_images\", \"input_image.jpg\"), frame) # save the input image\n",
    "        results, verified = verify(model, 0.5, 0.5) # verify the input image\n",
    "        print(verified)\n",
    "    \n",
    "    cv2.imshow('Verification', frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
